@wiki{wiki-renforcement,
	title = {Apprentissage par renforcement},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://fr.wikipedia.org/w/index.php?title=Apprentissage_par_renforcement&oldid=120752876},
	abstract = {L'apprentissage par renforcement fait référence à une classe de problèmes d'apprentissage automatique, dont le but est d'apprendre, à partir d'expériences, ce qu'il convient de faire en différentes situations, de façon à optimiser une récompense quantitative au cours du temps.
Un paradigme classique pour présenter les problèmes d'apprentissage par renforcement consiste à considérer un agent autonome, plongé au sein d'un environnement, et qui doit prendre des décisions en fonction de son état courant. En retour, l'environnement procure à l'agent une récompense, qui peut être positive ou négative.
L'agent cherche, au travers d'expériences itérées, un comportement décisionnel (appelé stratégie ou politique, et qui est une fonction associant à l'état courant l'action à exécuter) optimal, en ce sens qu'il maximise la somme des récompenses au cours du temps.},
	language = {fr},
	urldate = {2016-02-15},
	journal = {Wikipédia},
	month = nov,
	year = {2015},
	note = {Page Version ID: 120752876},
	file = {Snapshot:/home/thomas/.zotero/zotero/vqb76pmg.default/zotero/storage/XN5DVTCS/index.html:text/html}
}

@incollection{spring:classComp,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Learning {Intrusion} {Detection}: {Supervised} or {Unsupervised}?},
	copyright = {©2005 Springer-Verlag Berlin Heidelberg},
	isbn = {978-3-540-28869-5 978-3-540-31866-8},
	shorttitle = {Learning {Intrusion} {Detection}},
	url = {http://link.springer.com/chapter/10.1007/11553595_6},
	abstract = {Application and development of specialized machine learning techniques is gaining increasing attention in the intrusion detection community. A variety of learning techniques proposed for different intrusion detection problems can be roughly classified into two broad categories: supervised (classification) and unsupervised (anomaly detection and clustering). In this contribution we develop an experimental framework for comparative analysis of both kinds of learning techniques. In our framework we cast unsupervised techniques into a special case of classification, for which training and model selection can be performed by means of ROC analysis. We then investigate both kinds of learning techniques with respect to their detection accuracy and ability to detect unknown attacks.},
	language = {en},
	number = {3617},
	urldate = {2016-02-15},
	booktitle = {Image {Analysis} and {Processing} – {ICIAP} 2005},
	publisher = {Springer Berlin Heidelberg},
	author = {Laskov, Pavel and Düssel, Patrick and Schäfer, Christin and Rieck, Konrad},
	editor = {Roli, Fabio and Vitulano, Sergio},
	month = sep,
	year = {2005},
	note = {DOI: 10.1007/11553595\_6},
	keywords = {Algorithm Analysis and Problem Complexity, Artificial Intelligence (incl. Robotics), Computer Graphics, Image Processing and Computer Vision, Pattern Recognition},
	pages = {50--57},
	file = {Full Text PDF:/home/thomas/.zotero/zotero/vqb76pmg.default/zotero/storage/Q473SEAW/Laskov et al. - 2005 - Learning Intrusion Detection Supervised or Unsupe.pdf:application/pdf;Snapshot:/home/thomas/.zotero/zotero/vqb76pmg.default/zotero/storage/V95CQZVV/11553595_6.html:text/html}
}

@wiki{apprentissage_sp,
	title = {Apprentissage semi-supervisé},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://fr.wikipedia.org/w/index.php?title=Apprentissage_semi-supervis%C3%A9&oldid=119564802},
	abstract = {L'apprentissage semi-supervisé est une classe de techniques d'apprentissage automatique qui utilise un ensemble de données étiquetées et non-étiquetés. Il se situe ainsi entre l'apprentissage supervisé qui n'utilise que des données étiquetées et l'apprentissage non-supervisé qui n'utilise que des données non-étiquetées. Il a été démontré que l'utilisation de données non-étiquetées, en combinaison avec des données étiquetées, permet d'améliorer significativement la qualité de l'apprentissage. Un autre intérêt provient du fait que l'étiquetage de données nécessite l'intervention d'un utilisateur humain. Lorsque les jeux de données deviennent très grands, cette opération peut s'avérer fastidieuse. Dans ce cas, l'apprentissage semi-supervisé, qui ne nécessite que quelques étiquettes, revêt un intérêt pratique évident.
Un exemple d'apprentissage semi-supervisé est le coapprentissage, dans lequel deux classifieurs apprennent un ensemble de données, mais en utilisant chacun un ensemble de caractéristiques différentes, idéalement indépendantes. Si les données sont des individus à classer en hommes et femmes, l'un pourra utiliser la taille et l'autre la pilosité par exemple.},
	language = {fr},
	urldate = {2016-02-15},
	journal = {Wikipédia},
	month = oct,
	year = {2015},
	note = {Page Version ID: 119564802},
	file = {Snapshot:/home/thomas/.zotero/zotero/vqb76pmg.default/zotero/storage/P27U53EI/index.html:text/html}
}

@wiki{apprentissage_np,
	title = {Apprentissage non supervisé},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://fr.wikipedia.org/w/index.php?title=Apprentissage_non_supervis%C3%A9&oldid=123222753},
	abstract = {Dans le domaine informatique, l'apprentissage non supervisé (parfois dénommé « clustering ») est une méthode d'apprentissage automatique. Il s'agit pour un logiciel de diviser un groupe hétérogène de données, en sous-groupes de manière que les données considérées comme les plus similaires soient associées au sein d'un groupe homogène et qu'au contraire les données considérées comme différentes se retrouvent dans d'autres groupes distincts ; l'objectif étant de permettre une extraction de connaissance organisée à partir de ces données},
	language = {fr},
	urldate = {2016-02-15},
	journal = {Wikipédia},
	month = feb,
	year = {2016},
	note = {Page Version ID: 123222753},
	file = {Snapshot:/home/thomas/.zotero/zotero/vqb76pmg.default/zotero/storage/XAGUN2AU/index.html:text/html}
}

@wiki{apprentissage_s,
	title = {Apprentissage supervisé},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://fr.wikipedia.org/w/index.php?title=Apprentissage_supervis%C3%A9&oldid=123222743},
	abstract = {L'apprentissage supervisé (supervised learning en anglais) est une technique d'apprentissage automatique où l'on cherche à produire automatiquement des règles à partir d'une base de données d'apprentissage contenant des « exemples » (en général des cas déjà traités et validés).},
	language = {fr},
	urldate = {2016-02-15},
	journal = {Wikipédia},
	month = feb,
	year = {2016},
	note = {Page Version ID: 123222743},
	file = {Snapshot:/home/thomas/.zotero/zotero/vqb76pmg.default/zotero/storage/PSX93Q84/index.html:text/html}
}

@wiki{apprentissage_gen,
	title = {Apprentissage automatique},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://fr.wikipedia.org/w/index.php?title=Apprentissage_automatique&oldid=121369943},
	abstract = {L'apprentissage automatique ou apprentissage statistique (machine learning en anglais), champ d'étude de l'intelligence artificielle, concerne la conception, l'analyse, le développement et l'implémentation de méthodes permettant à une machine (au sens large) d'évoluer par un processus systématique, et ainsi de remplir des tâches difficiles ou impossibles à remplir par des moyens algorithmiques plus classiques.
L'analyse peut concerner des graphes, arbres, ou courbes (par exemple, la courbe d'évolution temporelle d'une mesure ; on parle alors de données continues, par opposition aux données discrètes associées à des attributs-valeurs classiques) au même titre que de simples grandeurs scalaires.
Un exemple possible d'apprentissage automatique est celui de la classification : étiqueter chaque donnée en l'associant à une classe. Différents systèmes d'apprentissage existent, listés ci-dessous.},
	language = {fr},
	urldate = {2016-02-15},
	journal = {Wikipédia},
	month = dec,
	year = {2015},
	note = {Page Version ID: 121369943},
	file = {Snapshot:/home/thomas/.zotero/zotero/vqb76pmg.default/zotero/storage/TVSNSZ4A/index.html:text/html}
}

@article{survey,
  title={A survey of learning-based techniques of email spam filtering},
  author={Blanzieri, Enrico and Bryl, Anton},
  journal={Artificial Intelligence Review},
  volume={29},
  number={1},
  pages={63--92},
  year={2008},
  publisher={Springer}
}


@wiki{wiki_bayesien,
	title = {Classification naïve bayésienne},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://fr.wikipedia.org/w/index.php?title=Classification_na%C3%AFve_bay%C3%A9sienne&oldid=122381109},
	abstract = {La classification naïve bayésienne est un type de classification Bayésienne probabiliste simple basée sur le théorème de Bayes avec une forte indépendance (dite naïve) des hypothèses. Elle met en œuvre un classifieur bayésien naïf, ou classifieur naïf de Bayes, appartenant à la famille des classifieurs linéaires.
Un terme plus approprié pour le modèle probabiliste sous-jacent pourrait être « modèle à caractéristiques statistiquement indépendantes ».
En termes simples, un classifieur bayésien naïf suppose que l'existence d'une caractéristique pour une classe, est indépendante de l'existence d'autres caractéristiques. Un fruit peut être considéré comme une pomme s'il est rouge, arrondi, et fait une dizaine de centimètres. Même si ces caractéristiques sont liées dans la réalité, un classifieur bayésien naïf déterminera que le fruit est une pomme en considérant indépendamment ces caractéristiques de couleur, de forme et de taille.
Selon la nature de chaque modèle probabiliste, les classifieurs bayésiens naïfs peuvent être entraînés efficacement dans un contexte d'apprentissage supervisé. Dans beaucoup d'applications pratiques, l'estimation des paramètres pour les modèles bayésiens naïfs repose sur le maximum de vraisemblance. Autrement dit, il est possible de travailler avec le modèle bayésien naïf sans se préoccuper de probabilité bayésienne ou utiliser les méthodes bayésiennes.
Malgré leur modèle de conception « naïf » et ses hypothèses de base extrêmement simplistes, les classifieurs bayésiens naïfs ont fait preuve d'une efficacité plus que suffisante dans beaucoup de situations réelles complexes. En 2004, un article a montré qu'il existe des raisons théoriques derrière cette efficacité inattendue. Toutefois, une autre étude de 2006 montre que des approches plus récentes (arbres renforcés, forêts aléatoires) permettent d'obtenir de meilleurs résultats.
L'avantage du classifieur bayésien naïf est qu'il requiert relativement peu de données d'entraînement pour estimer les paramètres nécessaires à la classification, à savoir moyennes et variances des différentes variables. En effet, l'hypothèse d'indépendance des variables permet de se contenter de la variance de chacune d'entre elle pour chaque classe, sans avoir à calculer de matrice de covariance.},
	language = {fr},
	urldate = {2016-02-15},
	journal = {Wikipédia},
	month = jan,
	year = {2016},
	note = {Page Version ID: 122381109},
	file = {Snapshot:/home/thomas/.zotero/zotero/vqb76pmg.default/zotero/storage/AZQXNCB9/index.html:text/html}
}


@wiki{wiki_randomforest,
	title = {Random forest},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Random_forest&oldid=700408350},
	abstract = {Random forests is a notion of the general technique of random decision forests that are an ensemble learning method for classification, regression and other tasks, that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Random decision forests correct for decision trees' habit of overfitting to their training set.
The algorithm for inducing Breiman's random forest was developed by Leo Breiman and Adele Cutler, and "Random Forests" is their trademark. The method combines Breiman's "bagging" idea and the random selection of features, introduced independently by Ho and Amit and Geman in order to construct a collection of decision trees with controlled variance.
The selection of a random subset of features is an example of the random subspace method, which, in Ho's formulation, is a way to implement the "stochastic discrimination" approach to classification proposed by Eugene Kleinberg.},
	language = {en},
	urldate = {2016-02-15},
	journal = {Wikipédia},
	month = jan,
	year = {2016},
	note = {Page Version ID: 700408350},
	file = {Snapshot:/home/thomas/.zotero/zotero/vqb76pmg.default/zotero/storage/DFJDCE5V/index.html:text/html}
}

